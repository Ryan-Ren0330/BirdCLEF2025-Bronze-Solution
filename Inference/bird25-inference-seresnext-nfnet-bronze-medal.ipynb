{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "Inference Only(LoadLocalTrainModel)\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ¥‰ BirdCLEF 2025 - Bronze Medal Solution (Private LB 0.893)\n",
    "\n",
    "### ðŸ“Œ Overview\n",
    "This notebook presents a **weighted ensemble** approach combining:\n",
    "- **ECA-NFNet-L0** (weight **0.6**)\n",
    "https://www.kaggle.com/datasets/myso1987/birdclef-2025-sed-models-p\n",
    "- **SeresNext26t_32x4d** (weight **0.4**)\n",
    "https://www.kaggle.com/datasets/i2nfinit3y/bird2025-sed-ckpt\n",
    "\n",
    "These models were chosen for their complementary strengths in detecting bird calls from noisy soundscapes. The ensemble improves robustness and generalization by leveraging diverse backbone architectures.  \n",
    "\n",
    "You may find this Notebook on Kaggle:\n",
    "https://www.kaggle.com/code/uoftryanren/bird25-inference-seresnext-nfnet-bronze-medal\n",
    "### ðŸ›  Key Techniques\n",
    "- Post-processing with **power adjustment**  \n",
    "  â†³ Based on [myso1987â€™s notebook](https://www.kaggle.com/code/myso1987/post-processing-with-power-adjustment-for-low-rank)  \n",
    "- SED model checkpoints from [Bird 2025 dataset](https://www.kaggle.com/datasets/i2nfinit3y/bird2025-sed-ckpt)  \n",
    "- **Attention-based pooling** and smart thresholding\n",
    "\n",
    "### ðŸ”’ Private LB Result\n",
    "- **Score**: `0.893`\n",
    "- **Medal**: ðŸ¥‰ Bronze  \n",
    "- **Note**: The ensemble weight (0.6/0.4) was optimized for Private LB. Further tuning might lead to overfitting; hence, be careful for changing parameters.\n",
    "\n",
    "### ðŸ“Ž Recommendations\n",
    "To maximize leaderboard performance:\n",
    "- Consider tuning weights privately per fold/model\n",
    "- Monitor public-private LB gap\n",
    "- Avoid hard-coding label order; always use taxonomy.csv\n",
    "\n",
    "---\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "ã€Šã€Šã€ŠSubmission1(Seresnext)ã€‹ã€‹ã€‹\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.501840Z",
     "iopub.status.busy": "2025-05-24T05:01:18.501493Z",
     "iopub.status.idle": "2025-05-24T05:01:18.506588Z",
     "shell.execute_reply": "2025-05-24T05:01:18.505434Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.501801Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import gc\n",
    "# import warnings\n",
    "# import logging\n",
    "# import time\n",
    "# import math\n",
    "# import cv2\n",
    "# from pathlib import Path\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import librosa\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import timm\n",
    "# from tqdm.auto import tqdm\n",
    "\n",
    "# warnings.filterwarnings(\"ignore\")\n",
    "# logging.basicConfig(level=logging.ERROR)\n",
    "# print(\"Finished import group000000000 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.508169Z",
     "iopub.status.busy": "2025-05-24T05:01:18.507779Z",
     "iopub.status.idle": "2025-05-24T05:01:18.526889Z",
     "shell.execute_reply": "2025-05-24T05:01:18.525909Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.508131Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class CFG:\n",
    " \n",
    "#     test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "#     submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "#     taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    \n",
    "#     # ------------------------------------------- #\n",
    "#     # [IMPORTANT]\n",
    "#     # * Melspectrogram & Audio Params\n",
    "#     # ------------------------------------------- #\n",
    "#     FS = 32000  \n",
    "#     WINDOW_SIZE = 5\n",
    "#     N_FFT = 2048\n",
    "#     HOP_LENGTH = 512\n",
    "#     N_MELS = 512\n",
    "#     FMIN = 20\n",
    "#     FMAX = 16000\n",
    "#     TARGET_SHAPE = (256, 256)\n",
    "\n",
    "#     # ------------------------------------------- #\n",
    "#     # * Model def\n",
    "#     # ------------------------------------------- #\n",
    "#     model_path = '/kaggle/input/bird25-d-330v2-ppv15-convnextv2-nano'\n",
    "#     model_name = 'convnextv2_nano.fcmae_ft_in22k_in1k'\n",
    "#     use_specific_folds = True\n",
    "#     folds = [0,1]\n",
    "#     in_channels = 1\n",
    "#     device = 'cpu'  \n",
    "    \n",
    "#     # Inference parameters\n",
    "#     batch_size = 16\n",
    "#     use_tta = False  \n",
    "#     tta_count = 3\n",
    "#     threshold = 0.5\n",
    "\n",
    "#     # util\n",
    "#     debug = False\n",
    "#     debug_count = 3\n",
    "\n",
    "# cfg = CFG()\n",
    "\n",
    "# print(f\"Using device: {cfg.device}\")\n",
    "# print(f\"Loading taxonomy data...\")\n",
    "# taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "# species_ids = taxonomy_df['primary_label'].tolist()\n",
    "# num_classes = len(species_ids)\n",
    "# print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.528304Z",
     "iopub.status.busy": "2025-05-24T05:01:18.527938Z",
     "iopub.status.idle": "2025-05-24T05:01:18.543295Z",
     "shell.execute_reply": "2025-05-24T05:01:18.541960Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.528271Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# class GeM(nn.Module):\n",
    "#     def __init__(self, p=3, eps=1e-6):\n",
    "#         super(GeM, self).__init__()\n",
    "#         self.p = nn.Parameter(torch.ones(1)*p)\n",
    "#         self.eps = eps\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.gem(x, p=self.p, eps=self.eps)\n",
    "\n",
    "#     def gem(self, x, p=3, eps=1e-6):\n",
    "#         return F.avg_pool2d(x.clamp(min=eps).pow(p), (x.size(-2), x.size(-1))).pow(1./p)\n",
    "\n",
    "#     def __repr__(self):\n",
    "#         return self.__class__.__name__ + \\\n",
    "#                 '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + \\\n",
    "#                 ', ' + 'eps=' + str(self.eps) + ')'\n",
    "# class BirdCLEFModel(nn.Module):\n",
    "#     def __init__(self, cfg, num_classes):\n",
    "#         super().__init__()\n",
    "#         self.cfg = cfg\n",
    "        \n",
    "#         self.backbone = timm.create_model(\n",
    "#             cfg.model_name,\n",
    "#             pretrained=False,  \n",
    "#             in_chans=cfg.in_channels,\n",
    "#             drop_rate=0.0,    \n",
    "#             drop_path_rate=0.0\n",
    "#         )\n",
    "        \n",
    "#         if 'efficientnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.classifier.in_features\n",
    "#             self.backbone.classifier = nn.Identity()\n",
    "#         elif 'resnet' in cfg.model_name:\n",
    "#             backbone_out = self.backbone.fc.in_features\n",
    "#             self.backbone.fc = nn.Identity()\n",
    "#         else:\n",
    "#             backbone_out = self.backbone.get_classifier().in_features\n",
    "#             self.backbone.reset_classifier(0, '')\n",
    "        \n",
    "#         self.pooling = nn.AdaptiveAvgPool2d(1)\n",
    "#         self.feat_dim = backbone_out\n",
    "#         self.classifier = nn.Linear(backbone_out, num_classes)\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         features = self.backbone(x)\n",
    "        \n",
    "#         if isinstance(features, dict):\n",
    "#             features = features['features']\n",
    "            \n",
    "#         if len(features.shape) == 4:\n",
    "#             features = self.pooling(features)\n",
    "#             features = features.view(features.size(0), -1)\n",
    "        \n",
    "#         logits = self.classifier(features)\n",
    "#         return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.578399Z",
     "iopub.status.busy": "2025-05-24T05:01:18.577969Z",
     "iopub.status.idle": "2025-05-24T05:01:18.585812Z",
     "shell.execute_reply": "2025-05-24T05:01:18.584693Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.578367Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def audio2melspec(audio_data, cfg):\n",
    "#     \"\"\"Convert audio data to mel spectrogram\"\"\"\n",
    "#     if np.isnan(audio_data).any():\n",
    "#         mean_signal = np.nanmean(audio_data)\n",
    "#         audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n",
    "\n",
    "#     mel_spec = librosa.feature.melspectrogram(\n",
    "#         y=audio_data,\n",
    "#         sr=cfg.FS,\n",
    "#         n_fft=cfg.N_FFT,\n",
    "#         hop_length=cfg.HOP_LENGTH,\n",
    "#         n_mels=cfg.N_MELS,\n",
    "#         fmin=cfg.FMIN,\n",
    "#         fmax=cfg.FMAX,\n",
    "#         power=2.0,\n",
    "#         pad_mode=\"reflect\",\n",
    "#         norm='slaney',\n",
    "#         htk=True,\n",
    "#         center=True,\n",
    "#     )\n",
    "\n",
    "#     mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n",
    "#     mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n",
    "    \n",
    "#     return mel_spec_norm\n",
    "\n",
    "# def process_audio_segment(audio_data, cfg):\n",
    "#     \"\"\"Process audio segment to get mel spectrogram\"\"\"\n",
    "#     if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n",
    "#         audio_data = np.pad(audio_data, \n",
    "#                           (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n",
    "#                           mode='constant')\n",
    "    \n",
    "#     mel_spec = audio2melspec(audio_data, cfg)\n",
    "    \n",
    "#     # Resize if needed\n",
    "#     if mel_spec.shape != cfg.TARGET_SHAPE:\n",
    "#         mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n",
    "        \n",
    "#     return mel_spec.astype(np.float32)\n",
    "    \n",
    "# def find_model_files(cfg):\n",
    "#     \"\"\"\n",
    "#     Find all .pth model files in the specified model directory\n",
    "#     \"\"\"\n",
    "#     model_files = []\n",
    "    \n",
    "#     model_dir = Path(cfg.model_path)\n",
    "    \n",
    "#     for path in model_dir.glob('**/*.pth'):\n",
    "#         model_files.append(str(path))\n",
    "    \n",
    "#     return model_files\n",
    "\n",
    "# def load_models(cfg, num_classes):\n",
    "#     \"\"\"\n",
    "#     Load all found model files and prepare them for ensemble\n",
    "#     \"\"\"\n",
    "#     models = []\n",
    "    \n",
    "#     model_files = find_model_files(cfg)\n",
    "    \n",
    "#     if not model_files:\n",
    "#         print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "#         return models\n",
    "    \n",
    "#     print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "#     if cfg.use_specific_folds:\n",
    "#         filtered_files = []\n",
    "#         for fold in cfg.folds:\n",
    "#             fold_files = [f for f in model_files if f\"fold{fold}\" in f]\n",
    "#             filtered_files.extend(fold_files)\n",
    "#         model_files = filtered_files\n",
    "#         print(f\"Using {len(model_files)} model files for the specified folds ({cfg.folds}).\")\n",
    "    \n",
    "#     for model_path in model_files:\n",
    "#         try:\n",
    "#             print(f\"Loading model: {model_path}\")\n",
    "#             checkpoint = torch.load(model_path, map_location=torch.device(cfg.device))\n",
    "            \n",
    "#             model = BirdCLEFModel(cfg, num_classes)\n",
    "#             model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#             model = model.to(cfg.device)\n",
    "#             model.eval()\n",
    "            \n",
    "#             models.append(model)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "#     return models\n",
    "\n",
    "# def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "#     \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "#     predictions = []\n",
    "#     row_ids = []\n",
    "#     soundscape_id = Path(audio_path).stem\n",
    "    \n",
    "#     try:\n",
    "#         print(f\"Processing {soundscape_id}\")\n",
    "#         audio_data, _ = librosa.load(audio_path, sr=cfg.FS)\n",
    "        \n",
    "#         total_segments = int(len(audio_data) / (cfg.FS * cfg.WINDOW_SIZE))\n",
    "        \n",
    "#         for segment_idx in range(total_segments):\n",
    "#             start_sample = segment_idx * cfg.FS * cfg.WINDOW_SIZE\n",
    "#             end_sample = start_sample + cfg.FS * cfg.WINDOW_SIZE\n",
    "#             segment_audio = audio_data[start_sample:end_sample]\n",
    "            \n",
    "#             end_time_sec = (segment_idx + 1) * cfg.WINDOW_SIZE\n",
    "#             row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "#             row_ids.append(row_id)\n",
    "\n",
    "#             if cfg.use_tta:\n",
    "#                 all_preds = []\n",
    "                \n",
    "#                 for tta_idx in range(cfg.tta_count):\n",
    "#                     mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "#                     mel_spec = apply_tta(mel_spec, tta_idx)\n",
    "\n",
    "#                     mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                     mel_spec = mel_spec.to(cfg.device)\n",
    "\n",
    "#                     if len(models) == 1:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = models[0](mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             all_preds.append(probs)\n",
    "#                     else:\n",
    "#                         segment_preds = []\n",
    "#                         for model in models:\n",
    "#                             with torch.no_grad():\n",
    "#                                 outputs = model(mel_spec)\n",
    "#                                 probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                                 segment_preds.append(probs)\n",
    "                        \n",
    "#                         avg_preds = np.mean(segment_preds, axis=0)\n",
    "#                         all_preds.append(avg_preds)\n",
    "\n",
    "#                 final_preds = np.mean(all_preds, axis=0)\n",
    "#             else:\n",
    "#                 mel_spec = process_audio_segment(segment_audio, cfg)\n",
    "                \n",
    "#                 mel_spec = torch.tensor(mel_spec, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "#                 mel_spec = mel_spec.to(cfg.device)\n",
    "                \n",
    "#                 if len(models) == 1:\n",
    "#                     with torch.no_grad():\n",
    "#                         outputs = models[0](mel_spec)\n",
    "#                         final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                 else:\n",
    "#                     segment_preds = []\n",
    "#                     for model in models:\n",
    "#                         with torch.no_grad():\n",
    "#                             outputs = model(mel_spec)\n",
    "#                             probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "#                             segment_preds.append(probs)\n",
    "\n",
    "#                     final_preds = np.mean(segment_preds, axis=0)\n",
    "                    \n",
    "#             predictions.append(final_preds)\n",
    "            \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing {audio_path}: {e}\")\n",
    "    \n",
    "#     return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.587780Z",
     "iopub.status.busy": "2025-05-24T05:01:18.587416Z",
     "iopub.status.idle": "2025-05-24T05:01:18.605751Z",
     "shell.execute_reply": "2025-05-24T05:01:18.604853Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.587752Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def apply_tta(spec, tta_idx):\n",
    "#     \"\"\"Apply test-time augmentation\"\"\"\n",
    "#     if tta_idx == 0:\n",
    "#         # Original spectrogram\n",
    "#         return spec\n",
    "#     elif tta_idx == 1:\n",
    "#         # Time shift (horizontal flip)\n",
    "#         return np.flip(spec, axis=1)\n",
    "#     elif tta_idx == 2:\n",
    "#         # Frequency shift (vertical flip)\n",
    "#         return np.flip(spec, axis=0)\n",
    "#     else:\n",
    "#         return spec\n",
    "\n",
    "# def run_inference(cfg, models, species_ids):\n",
    "#     \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "#     test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    \n",
    "#     if cfg.debug:\n",
    "#         print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n",
    "#         test_files = test_files[:cfg.debug_count]\n",
    "    \n",
    "#     print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "#     all_row_ids = []\n",
    "#     all_predictions = []\n",
    "\n",
    "#     for audio_path in tqdm(test_files):\n",
    "#         row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n",
    "#         all_row_ids.extend(row_ids)\n",
    "#         all_predictions.extend(predictions)\n",
    "    \n",
    "#     return all_row_ids, all_predictions\n",
    "\n",
    "# def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "#     \"\"\"Create submission dataframe\"\"\"\n",
    "#     print(\"Creating submission dataframe...\")\n",
    "\n",
    "#     submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "#     for i, species in enumerate(species_ids):\n",
    "#         submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "#     submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "#     submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "#     sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "#     missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "#     if missing_cols:\n",
    "#         print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "#         for col in missing_cols:\n",
    "#             submission_df[col] = 0.0\n",
    "\n",
    "#     submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "#     submission_df = submission_df.reset_index()\n",
    "    \n",
    "#     return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.608458Z",
     "iopub.status.busy": "2025-05-24T05:01:18.607725Z",
     "iopub.status.idle": "2025-05-24T05:01:18.623901Z",
     "shell.execute_reply": "2025-05-24T05:01:18.622726Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.608408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     start_time = time.time()\n",
    "#     print(\"Starting BirdCLEF-2025 inference...\")\n",
    "#     print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n",
    "\n",
    "#     models = load_models(cfg, num_classes)\n",
    "    \n",
    "#     if not models:\n",
    "#         print(\"No models found! Please check model paths.\")\n",
    "#         return\n",
    "    \n",
    "#     print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "#     row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "#     submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "#     submission_path = 'submission1.csv'\n",
    "#     submission_df.to_csv(submission_path, index=False)\n",
    "#     print(f\"Submission saved to {submission_path}\")\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.625594Z",
     "iopub.status.busy": "2025-05-24T05:01:18.625245Z",
     "iopub.status.idle": "2025-05-24T05:01:18.638651Z",
     "shell.execute_reply": "2025-05-24T05:01:18.637478Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.625561Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# sub = pd.read_csv('submission1.csv')\n",
    "# cols = sub.columns[1:]\n",
    "# groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "# groups = groups.values\n",
    "# for group in np.unique(groups):\n",
    "#     sub_group = sub[group == groups]\n",
    "#     predictions = sub_group[cols].values\n",
    "#     new_predictions = predictions.copy()\n",
    "#     for i in range(1, predictions.shape[0]-1):\n",
    "#         new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "#     new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "#     new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "#     sub_group[cols] = new_predictions\n",
    "#     sub[group == groups] = sub_group\n",
    "# sub.to_csv(\"submission1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:18.640191Z",
     "iopub.status.busy": "2025-05-24T05:01:18.639811Z",
     "iopub.status.idle": "2025-05-24T05:01:31.986870Z",
     "shell.execute_reply": "2025-05-24T05:01:31.985734Z",
     "shell.execute_reply.started": "2025-05-24T05:01:18.640162Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import logging\n",
    "import time\n",
    "import math\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from soundfile import SoundFile \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import timm\n",
    "from tqdm.auto import tqdm\n",
    "from glob import glob\n",
    "import torchaudio\n",
    "import random\n",
    "import itertools\n",
    "from typing import Union\n",
    "\n",
    "import concurrent.futures\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "logging.basicConfig(level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:31.988327Z",
     "iopub.status.busy": "2025-05-24T05:01:31.987931Z",
     "iopub.status.idle": "2025-05-24T05:01:31.994257Z",
     "shell.execute_reply": "2025-05-24T05:01:31.992879Z",
     "shell.execute_reply.started": "2025-05-24T05:01:31.988292Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CFG:\n",
    "    \n",
    "    seed = 42\n",
    "    print_freq = 100\n",
    "    num_workers = 4\n",
    "\n",
    "    stage = 'train_bce'\n",
    "\n",
    "    train_datadir = '/kaggle/input/birdclef-2025/train_audio'\n",
    "    train_csv = '/kaggle/input/birdclef-2025/train.csv'\n",
    "    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n",
    "    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n",
    "    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n",
    "    model_files = ['/kaggle/input/bird2025-sed-ckpt/sedmodel.pth'\n",
    "                  ]\n",
    " \n",
    "    model_name = 'seresnext26t_32x4d'  \n",
    "    pretrained = False\n",
    "    in_channels = 1\n",
    "\n",
    "    \n",
    "    SR = 32000\n",
    "    target_duration = 5\n",
    "    train_duration = 10\n",
    "    \n",
    "    \n",
    "    device = 'cpu'\n",
    "\n",
    "cfg = CFG()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:31.996078Z",
     "iopub.status.busy": "2025-05-24T05:01:31.995643Z",
     "iopub.status.idle": "2025-05-24T05:01:32.035378Z",
     "shell.execute_reply": "2025-05-24T05:01:32.034276Z",
     "shell.execute_reply.started": "2025-05-24T05:01:31.996031Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Using device: {cfg.device}\")\n",
    "print(f\"Loading taxonomy data...\")\n",
    "taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n",
    "species_ids = taxonomy_df['primary_label'].tolist()\n",
    "num_classes = len(species_ids)\n",
    "print(f\"Number of classes: {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.038809Z",
     "iopub.status.busy": "2025-05-24T05:01:32.038519Z",
     "iopub.status.idle": "2025-05-24T05:01:32.050630Z",
     "shell.execute_reply": "2025-05-24T05:01:32.049690Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.038785Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(cfg.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.052806Z",
     "iopub.status.busy": "2025-05-24T05:01:32.052500Z",
     "iopub.status.idle": "2025-05-24T05:01:32.061736Z",
     "shell.execute_reply": "2025-05-24T05:01:32.060543Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.052779Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (n_samples, n_in, n_time)\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == \"linear\":\n",
    "            return x\n",
    "        elif self.activation == \"sigmoid\":\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.0)\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.0)\n",
    "    bn.weight.data.fill_(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.063330Z",
     "iopub.status.busy": "2025-05-24T05:01:32.062966Z",
     "iopub.status.idle": "2025-05-24T05:01:32.091073Z",
     "shell.execute_reply": "2025-05-24T05:01:32.089971Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.063304Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class BirdCLEFModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        taxonomy_df = pd.read_csv('/kaggle/input/birdclef-2025/taxonomy.csv')\n",
    "        self.num_classes = len(taxonomy_df)\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(cfg['n_mels'])\n",
    "        \n",
    "        self.backbone = timm.create_model(\n",
    "            cfg['model_name'],\n",
    "            pretrained=False,\n",
    "            in_chans=cfg['in_channels'],\n",
    "            drop_rate=0.2,\n",
    "            drop_path_rate=0.2,\n",
    "        )\n",
    "\n",
    "        layers = list(self.backbone.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "        \n",
    "        if \"efficientnet\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.classifier.in_features\n",
    "        elif \"eca\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.head.fc.in_features\n",
    "        elif \"res\" in self.cfg['model_name']:\n",
    "            backbone_out = self.backbone.fc.in_features\n",
    "        else:\n",
    "            backbone_out = self.backbone.num_features\n",
    "            \n",
    "        \n",
    "        self.fc1 = nn.Linear(backbone_out, backbone_out, bias=True)\n",
    "        self.att_block = AttBlockV2(backbone_out, self.num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.melspec_transform = torchaudio.transforms.MelSpectrogram(\n",
    "            sample_rate=self.cfg['SR'],\n",
    "            hop_length=self.cfg['hop_length'],\n",
    "            n_mels=self.cfg['n_mels'],\n",
    "            f_min=self.cfg['f_min'],\n",
    "            f_max=self.cfg['f_max'],\n",
    "            n_fft=self.cfg['n_fft'],\n",
    "            pad_mode=\"constant\",\n",
    "            norm=\"slaney\",\n",
    "            onesided=True,\n",
    "            mel_scale=\"htk\",\n",
    "        )\n",
    "        if self.cfg['device'] == \"cuda\":\n",
    "            self.melspec_transform = self.melspec_transform.cuda()\n",
    "        else:\n",
    "            self.melspec_transform = self.melspec_transform.cpu()\n",
    "\n",
    "        self.db_transform = torchaudio.transforms.AmplitudeToDB(\n",
    "            stype=\"power\", top_db=80\n",
    "        )\n",
    "\n",
    "\n",
    "    def extract_feature(self,x):\n",
    "        x = x.permute((0, 1, 3, 2))\n",
    "        frames_num = x.shape[2]\n",
    "        \n",
    "        x = x.transpose(1, 3)\n",
    "        x = self.bn0(x)\n",
    "        x = x.transpose(1, 3)\n",
    "        \n",
    "        # if self.training:\n",
    "        #    x = self.spec_augmenter(x)\n",
    "        \n",
    "        x = x.transpose(2, 3)\n",
    "        # (batch_size, channels, freq, frames)\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # (batch_size, channels, frames)\n",
    "        x = torch.mean(x, dim=2)\n",
    "        \n",
    "        # channel smoothing\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "        \n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return x, frames_num\n",
    "        \n",
    "    @torch.cuda.amp.autocast(enabled=False)\n",
    "    def transform_to_spec(self, audio):\n",
    "\n",
    "        audio = audio.float()\n",
    "        \n",
    "        spec = self.melspec_transform(audio)\n",
    "        spec = self.db_transform(spec)\n",
    "\n",
    "        if self.cfg['normal'] == 80:\n",
    "            spec = (spec + 80) / 80\n",
    "        elif self.cfg['normal'] == 255:\n",
    "            spec = spec / 255\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "                \n",
    "        if self.cfg['in_channels'] == 3:\n",
    "            spec = image_delta(spec)\n",
    "        \n",
    "        return spec\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "\n",
    "        x, frames_num = self.extract_feature(x)\n",
    "        \n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block(x)\n",
    "        logit = torch.sum(norm_att * self.att_block.cla(x), dim=2)\n",
    "        segmentwise_logit = self.att_block.cla(x).transpose(1, 2)\n",
    "        segmentwise_output = segmentwise_output.transpose(1, 2)\n",
    "\n",
    "        return torch.logit(clipwise_output)\n",
    "\n",
    "    def infer(self, x, tta_delta=2):\n",
    "        with torch.no_grad():\n",
    "            x = self.transform_to_spec(x)\n",
    "        x,_ = self.extract_feature(x)\n",
    "        time_att = torch.tanh(self.att_block.att(x))\n",
    "        feat_time = x.size(-1)\n",
    "        start = (\n",
    "            feat_time / 2 - feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train']) / 2\n",
    "        )\n",
    "        end = start + feat_time * (self.cfg['infer_duration'] / self.cfg['duration_train'])\n",
    "        start = int(start)\n",
    "        end = int(end)\n",
    "        pred = self.attention_infer(start,end,x,time_att)\n",
    "\n",
    "        start_minus = max(0, start-tta_delta)\n",
    "        end_minus=end-tta_delta\n",
    "        pred_minus = self.attention_infer(start_minus,end_minus,x,time_att)\n",
    "\n",
    "        start_plus = start+tta_delta\n",
    "        end_plus=min(feat_time, end+tta_delta)\n",
    "        pred_plus = self.attention_infer(start_plus,end_plus,x,time_att)\n",
    "\n",
    "        pred = 0.5*pred + 0.25*pred_minus + 0.25*pred_plus\n",
    "        return pred\n",
    "        \n",
    "    def attention_infer(self,start,end,x,time_att):\n",
    "        feat = x[:, :, start:end]\n",
    "        # att = torch.softmax(time_att[:, :, start:end], dim=-1)\n",
    "        #             print(feat_time, start, end)\n",
    "        #             print(att_a.sum(), att.sum(), time_att.shape)\n",
    "        framewise_pred = torch.sigmoid(self.att_block.cla(feat))\n",
    "        framewise_pred_max = framewise_pred.max(dim=2)[0]\n",
    "        # clipwise_output = torch.sum(framewise_pred * att, dim=-1)\n",
    "        #logits = torch.sum(\n",
    "        #    self.att_block.cla(feat) * att,\n",
    "        #    dim=-1,\n",
    "        #)\n",
    "\n",
    "        # return clipwise_output\n",
    "        return framewise_pred_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.092652Z",
     "iopub.status.busy": "2025-05-24T05:01:32.092266Z",
     "iopub.status.idle": "2025-05-24T05:01:32.115454Z",
     "shell.execute_reply": "2025-05-24T05:01:32.114528Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.092614Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def load_sample(path, cfg):\n",
    "    audio, orig_sr = sf.read(path, dtype=\"float32\")\n",
    "    seconds = []\n",
    "    audio_length = cfg.SR * cfg.target_duration\n",
    "    step = audio_length\n",
    "    for i in range(audio_length, len(audio) + step, step):\n",
    "        start = max(0, i - audio_length)\n",
    "        end = start + audio_length\n",
    "        if end > len(audio):\n",
    "            pass\n",
    "        else:\n",
    "            seconds.append(int(end/cfg.SR))\n",
    "\n",
    "    audio = np.concatenate([audio,audio,audio])\n",
    "    audios = []\n",
    "    for i,second in enumerate(seconds):\n",
    "        end_seconds = int(second)\n",
    "        start_seconds = int(end_seconds - cfg.target_duration)\n",
    "    \n",
    "        end_index = int(cfg.SR * (end_seconds + (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        start_index = int(cfg.SR * (start_seconds - (cfg.train_duration - cfg.target_duration) / 2) ) + len(audio) // 3\n",
    "        end_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        start_pad = int(cfg.SR * (cfg.train_duration - cfg.target_duration) / 2) \n",
    "        y = audio[start_index:end_index].astype(np.float32)\n",
    "        if i==0:\n",
    "            y[:start_pad] = 0\n",
    "        elif i==(len(seconds)-1):\n",
    "            y[-end_pad:] = 0\n",
    "        audios.append(y)\n",
    "\n",
    "    return audios\n",
    "\n",
    "def sigmoid(x):\n",
    "    s = 1 / (1 + np.exp(-x))\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.116862Z",
     "iopub.status.busy": "2025-05-24T05:01:32.116579Z",
     "iopub.status.idle": "2025-05-24T05:01:32.135398Z",
     "shell.execute_reply": "2025-05-24T05:01:32.134528Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.116839Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def find_model_files(cfg):\n",
    "    \"\"\"\n",
    "    Find all .pth model files in the specified model directory\n",
    "    \"\"\"\n",
    "    model_files = []\n",
    "    \n",
    "    model_dir = Path(cfg.model_path)\n",
    "    \n",
    "    for path in model_dir.glob('**/*.pth'):\n",
    "        model_files.append(str(path))\n",
    "    \n",
    "    return model_files\n",
    "\n",
    "def load_models(cfg, num_classes):\n",
    "    \"\"\"\n",
    "    Load all found model files and prepare them for ensemble\n",
    "    \"\"\"\n",
    "    models = []\n",
    "    \n",
    "    # model_files = find_model_files(cfg)\n",
    "    model_files = cfg.model_files\n",
    "    \n",
    "    if not model_files:\n",
    "        print(f\"Warning: No model files found under {cfg.model_path}!\")\n",
    "        return models\n",
    "    \n",
    "    print(f\"Found a total of {len(model_files)} model files.\")\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        try:\n",
    "            print(f\"Loading model: {model_path}\")\n",
    "            checkpoint = torch.load(model_path, map_location=torch.device(cfg.device), weights_only=False)\n",
    "            cfg_temp = checkpoint['cfg']\n",
    "            cfg_temp['device'] = cfg.device\n",
    "            \n",
    "            model = BirdCLEFModel(cfg_temp)\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "            model = model.to(cfg.device)\n",
    "            model.eval()\n",
    "            model.zero_grad()\n",
    "            model.half().float()\n",
    "            \n",
    "            models.append(model)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model {model_path}: {e}\")\n",
    "    \n",
    "    return models\n",
    "\n",
    "def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n",
    "    \"\"\"Process a single audio file and predict species presence for each 5-second segment\"\"\"\n",
    "    audio_path = str(audio_path)\n",
    "    predictions = []\n",
    "    row_ids = []\n",
    "    soundscape_id = Path(audio_path).stem\n",
    "\n",
    "    print(f\"Processing {soundscape_id}\")\n",
    "    audio_data = load_sample(audio_path, cfg)\n",
    "    for segment_idx, audio_input in enumerate(audio_data):\n",
    "        \n",
    "        end_time_sec = (segment_idx + 1) * cfg.target_duration\n",
    "        row_id = f\"{soundscape_id}_{end_time_sec}\"\n",
    "        row_ids.append(row_id)\n",
    "        \n",
    "        mel_spec = torch.tensor(audio_input, dtype=torch.float32).unsqueeze(0).unsqueeze(0)\n",
    "        mel_spec = mel_spec.to(cfg.device)\n",
    "        \n",
    "        if len(models) == 1:\n",
    "            with torch.no_grad():\n",
    "                outputs = models[0].infer(mel_spec)\n",
    "                final_preds = outputs.squeeze()\n",
    "                # final_preds = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "\n",
    "        else:\n",
    "            segment_preds = []\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    outputs = model.infer(mel_spec)\n",
    "                    probs = outputs.squeeze()\n",
    "                    # probs = torch.sigmoid(outputs).cpu().numpy().squeeze()\n",
    "                    segment_preds.append(probs)\n",
    "\n",
    "            \n",
    "            final_preds = np.mean(segment_preds, axis=0)\n",
    "                \n",
    "        predictions.append(final_preds)\n",
    "\n",
    "    predictions = np.stack(predictions,axis=0)\n",
    "    \n",
    "    return row_ids, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.136737Z",
     "iopub.status.busy": "2025-05-24T05:01:32.136390Z",
     "iopub.status.idle": "2025-05-24T05:01:32.156157Z",
     "shell.execute_reply": "2025-05-24T05:01:32.155092Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.136696Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def run_inference(cfg, models, species_ids):\n",
    "    \"\"\"Run inference on all test soundscapes\"\"\"\n",
    "    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n",
    "    if len(test_files) == 0:\n",
    "        test_files = sorted(glob(str(Path('/kaggle/input/birdclef-2025/train_soundscapes') / '*.ogg')))[:10]\n",
    "    \n",
    "    print(f\"Found {len(test_files)} test soundscapes\")\n",
    "\n",
    "    all_row_ids = []\n",
    "    all_predictions = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:\n",
    "        results = list(\n",
    "        executor.map(\n",
    "            predict_on_spectrogram,\n",
    "            test_files,\n",
    "            itertools.repeat(models),\n",
    "            itertools.repeat(cfg),\n",
    "            itertools.repeat(species_ids)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    for rids, preds in results:\n",
    "        all_row_ids.extend(rids)\n",
    "        all_predictions.extend(preds)\n",
    "    \n",
    "    return all_row_ids, all_predictions\n",
    "\n",
    "def create_submission(row_ids, predictions, species_ids, cfg):\n",
    "    \"\"\"Create submission dataframe\"\"\"\n",
    "    print(\"Creating submission dataframe...\")\n",
    "\n",
    "    submission_dict = {'row_id': row_ids}\n",
    "    \n",
    "    for i, species in enumerate(species_ids):\n",
    "        submission_dict[species] = [pred[i] for pred in predictions]\n",
    "\n",
    "    submission_df = pd.DataFrame(submission_dict)\n",
    "\n",
    "    submission_df.set_index('row_id', inplace=True)\n",
    "\n",
    "    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n",
    "\n",
    "    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n",
    "    if missing_cols:\n",
    "        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n",
    "        for col in missing_cols:\n",
    "            submission_df[col] = 0.0\n",
    "\n",
    "    submission_df = submission_df[sample_sub.columns]\n",
    "\n",
    "    submission_df = submission_df.reset_index()\n",
    "    \n",
    "    return submission_df\n",
    "\n",
    "\n",
    "def smooth_submission(submission_path):\n",
    "        \"\"\"\n",
    "        Post-process the submission CSV by smoothing predictions to enforce temporal consistency.\n",
    "        \n",
    "        For each soundscape (grouped by the file name part of 'row_id'), each row's predictions\n",
    "        are averaged with those of its neighbors using defined weights.\n",
    "        \n",
    "        :param submission_path: Path to the submission CSV file.\n",
    "        \"\"\"\n",
    "        print(\"Smoothing submission predictions...\")\n",
    "        sub = pd.read_csv(submission_path)\n",
    "        cols = sub.columns[1:]\n",
    "        # Extract group names by splitting row_id on the last underscore\n",
    "        groups = sub['row_id'].str.rsplit('_', n=1).str[0].values\n",
    "        unique_groups = np.unique(groups)\n",
    "        \n",
    "        for group in unique_groups:\n",
    "            # Get indices for the current group\n",
    "            idx = np.where(groups == group)[0]\n",
    "            sub_group = sub.iloc[idx].copy()\n",
    "            predictions = sub_group[cols].values\n",
    "            new_predictions = predictions.copy()\n",
    "            \n",
    "            if predictions.shape[0] > 1:\n",
    "                # Smooth the predictions using neighboring segments\n",
    "                new_predictions[0] = (predictions[0] * 0.8) + (predictions[1] * 0.2)\n",
    "                new_predictions[-1] = (predictions[-1] * 0.8) + (predictions[-2] * 0.2)\n",
    "                for i in range(1, predictions.shape[0]-1):\n",
    "                    new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "            # Replace the smoothed values in the submission dataframe\n",
    "            sub.iloc[idx, 1:] = new_predictions\n",
    "        \n",
    "        sub.to_csv(submission_path, index=False)\n",
    "        print(f\"Smoothed submission saved to {submission_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.157444Z",
     "iopub.status.busy": "2025-05-24T05:01:32.157147Z",
     "iopub.status.idle": "2025-05-24T05:01:32.172032Z",
     "shell.execute_reply": "2025-05-24T05:01:32.170877Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.157418Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    start_time = time.time()\n",
    "    print(\"Starting BirdCLEF-2025 inference...\")\n",
    "\n",
    "    models = load_models(cfg, num_classes)\n",
    "    \n",
    "    if not models:\n",
    "        print(\"No models found! Please check model paths.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n",
    "\n",
    "    row_ids, predictions = run_inference(cfg, models, species_ids)\n",
    "\n",
    "    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n",
    "\n",
    "    submission_path = 'submission1.csv'\n",
    "    submission_df.to_csv(submission_path, index=False)\n",
    "    print(f\"Submission saved to {submission_path}\")\n",
    "\n",
    "    smooth_submission(submission_path)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:01:32.173539Z",
     "iopub.status.busy": "2025-05-24T05:01:32.173191Z",
     "iopub.status.idle": "2025-05-24T05:02:10.741164Z",
     "shell.execute_reply": "2025-05-24T05:02:10.740081Z",
     "shell.execute_reply.started": "2025-05-24T05:01:32.173512Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "ã€Šã€Šã€ŠSubmission2(nfnet)ã€‹ã€‹ã€‹\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.742407Z",
     "iopub.status.busy": "2025-05-24T05:02:10.742116Z",
     "iopub.status.idle": "2025-05-24T05:02:10.749513Z",
     "shell.execute_reply": "2025-05-24T05:02:10.748318Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.742385Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Union\n",
    "\n",
    "def apply_power_to_low_ranked_cols(\n",
    "    p: np.ndarray,\n",
    "    top_k: int = 30,\n",
    "    exponent: Union[int, float] = 2,\n",
    "    inplace: bool = True\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Rank columns by their columnâ€‘wise maximum and raise every column whose\n",
    "    rank falls below `top_k` to a given power.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    p : np.ndarray\n",
    "        A 2â€‘D array of shape **(n_chunks, n_classes)**.\n",
    "\n",
    "        - **n_chunks** is the number of fixedâ€‘length time chunks obtained\n",
    "          after slicing the input audio (or other sequential data).  \n",
    "          *Example:* In the BirdCLEF `test_soundscapes` set, each file is\n",
    "          60â€¯s long. If you extract nonâ€‘overlapping 5â€¯s windows,  \n",
    "          `n_chunks = 60â€¯s / 5â€¯s = 12`.\n",
    "        - **n_classes** is the number of classes being predicted.\n",
    "        - Each element `p[i, j]` is the score or probability of class *j*\n",
    "          in chunk *i*.\n",
    "\n",
    "    top_k : int, default=30\n",
    "        The highestâ€‘ranked columns (by their maximum value) that remain\n",
    "        unchanged.\n",
    "\n",
    "    exponent : int or float, default=2\n",
    "        The power applied to the selected lowâ€‘ranked columns  \n",
    "        (e.g. `2` squares, `0.5` takes the square root, `3` cubes).\n",
    "\n",
    "    inplace : bool, default=True\n",
    "        If `True`, modify `p` in place.  \n",
    "        If `False`, operate on a copy and leave the original array intact.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "        The transformed array. It is the same object as `p` when\n",
    "        `inplace=True`; otherwise, it is a new array.\n",
    "\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        p = p.copy()\n",
    "\n",
    "    # Identify columns whose max value ranks below `top_k`\n",
    "    tail_cols = np.argsort(-p.max(axis=0))[top_k:]\n",
    "\n",
    "    # Apply the power transformation to those columns\n",
    "    p[:, tail_cols] = p[:, tail_cols] ** exponent\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.750983Z",
     "iopub.status.busy": "2025-05-24T05:02:10.750698Z",
     "iopub.status.idle": "2025-05-24T05:02:10.773394Z",
     "shell.execute_reply": "2025-05-24T05:02:10.772270Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.750958Z"
    },
    "papermill": {
     "duration": 12.984639,
     "end_time": "2025-03-12T14:13:00.145177",
     "exception": false,
     "start_time": "2025-03-12T14:12:47.160538",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import torch.nn.functional as F\n",
    "import torchaudio\n",
    "import torchaudio.transforms as AT\n",
    "from contextlib import contextmanager\n",
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.774783Z",
     "iopub.status.busy": "2025-05-24T05:02:10.774429Z",
     "iopub.status.idle": "2025-05-24T05:02:10.795256Z",
     "shell.execute_reply": "2025-05-24T05:02:10.794110Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.774745Z"
    },
    "papermill": {
     "duration": 0.105385,
     "end_time": "2025-03-12T14:13:00.253425",
     "exception": false,
     "start_time": "2025-03-12T14:13:00.14804",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "test_audio_dir = '../input/birdclef-2025/test_soundscapes/'\n",
    "file_list = [f for f in sorted(os.listdir(test_audio_dir))]\n",
    "file_list = [file.split('.')[0] for file in file_list if file.endswith('.ogg')]\n",
    "\n",
    "debug = False\n",
    "print('Debug mode:', debug)\n",
    "print('Number of test soundscapes:', len(file_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.796829Z",
     "iopub.status.busy": "2025-05-24T05:02:10.796409Z",
     "iopub.status.idle": "2025-05-24T05:02:10.818632Z",
     "shell.execute_reply": "2025-05-24T05:02:10.817769Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.796789Z"
    },
    "papermill": {
     "duration": 0.144235,
     "end_time": "2025-03-12T14:13:00.400505",
     "exception": false,
     "start_time": "2025-03-12T14:13:00.25627",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wav_sec = 5\n",
    "sample_rate = 32000\n",
    "min_segment = sample_rate*wav_sec\n",
    "\n",
    "class_labels = sorted(os.listdir('../input/birdclef-2025/train_audio/'))\n",
    "\n",
    "n_fft=1024\n",
    "win_length=1024\n",
    "hop_length=512\n",
    "f_min=50\n",
    "f_max=16000\n",
    "n_mels=128\n",
    "\n",
    "mel_spectrogram = AT.MelSpectrogram(\n",
    "    sample_rate=sample_rate,\n",
    "    n_fft=n_fft,\n",
    "    win_length=win_length,\n",
    "    hop_length=hop_length,\n",
    "    center=True,\n",
    "    f_min=f_min,\n",
    "    f_max=f_max,\n",
    "    pad_mode=\"reflect\",\n",
    "    power=2.0,\n",
    "    norm='slaney',\n",
    "    n_mels=n_mels,\n",
    "    mel_scale=\"htk\",\n",
    "    # normalized=True\n",
    ")\n",
    "\n",
    "def normalize_std(spec, eps=1e-6):\n",
    "    mean = torch.mean(spec)\n",
    "    std = torch.std(spec)\n",
    "    return torch.where(std == 0, spec-mean, (spec - mean) / (std+eps))\n",
    "\n",
    "def audio_to_mel(filepath=None):\n",
    "    waveform, sample_rate = torchaudio.load(filepath,backend=\"soundfile\")\n",
    "    len_wav = waveform.shape[1]\n",
    "    waveform = waveform[0,:].reshape(1, len_wav) # stereo->mono mono->mono\n",
    "    PREDS = []\n",
    "    for i in range(12):\n",
    "        waveform2 = waveform[:,i*sample_rate*5:i*sample_rate*5+sample_rate*5]\n",
    "        melspec = mel_spectrogram(waveform2)\n",
    "        melspec = torch.log(melspec+1e-6)\n",
    "        melspec = normalize_std(melspec)\n",
    "        melspec = torch.unsqueeze(melspec, dim=0)\n",
    "        \n",
    "        PREDS.append(melspec)\n",
    "    return torch.vstack(PREDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.819580Z",
     "iopub.status.busy": "2025-05-24T05:02:10.819301Z",
     "iopub.status.idle": "2025-05-24T05:02:10.839936Z",
     "shell.execute_reply": "2025-05-24T05:02:10.838611Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.819556Z"
    },
    "papermill": {
     "duration": 2.175154,
     "end_time": "2025-03-12T14:13:02.578522",
     "exception": false,
     "start_time": "2025-03-12T14:13:00.403368",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_layer(layer):\n",
    "    nn.init.xavier_uniform_(layer.weight)\n",
    "    if hasattr(layer, \"bias\"):\n",
    "        if layer.bias is not None:\n",
    "            layer.bias.data.fill_(0.)\n",
    "\n",
    "\n",
    "def init_bn(bn):\n",
    "    bn.bias.data.fill_(0.)\n",
    "    bn.weight.data.fill_(1.0)\n",
    "\n",
    "\n",
    "def init_weights(model):\n",
    "    classname = model.__class__.__name__\n",
    "    if classname.find(\"Conv2d\") != -1:\n",
    "        nn.init.xavier_uniform_(model.weight, gain=np.sqrt(2))\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"BatchNorm\") != -1:\n",
    "        model.weight.data.normal_(1.0, 0.02)\n",
    "        model.bias.data.fill_(0)\n",
    "    elif classname.find(\"GRU\") != -1:\n",
    "        for weight in model.parameters():\n",
    "            if len(weight.size()) > 1:\n",
    "                nn.init.orghogonal_(weight.data)\n",
    "    elif classname.find(\"Linear\") != -1:\n",
    "        model.weight.data.normal_(0, 0.01)\n",
    "        model.bias.data.zero_()\n",
    "\n",
    "\n",
    "def interpolate(x, ratio):\n",
    "    (batch_size, time_steps, classes_num) = x.shape\n",
    "    upsampled = x[:, :, None, :].repeat(1, 1, ratio, 1)\n",
    "    upsampled = upsampled.reshape(batch_size, time_steps * ratio, classes_num)\n",
    "    return upsampled\n",
    "\n",
    "\n",
    "def pad_framewise_output(framewise_output, frames_num):\n",
    "    output = F.interpolate(\n",
    "        framewise_output.unsqueeze(1),\n",
    "        size=(frames_num, framewise_output.size(2)),\n",
    "        align_corners=True,\n",
    "        mode=\"bilinear\").squeeze(1)\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AttBlockV2(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 activation=\"linear\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.activation = activation\n",
    "        self.att = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "        self.cla = nn.Conv1d(\n",
    "            in_channels=in_features,\n",
    "            out_channels=out_features,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0,\n",
    "            bias=True)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        init_layer(self.att)\n",
    "        init_layer(self.cla)\n",
    "\n",
    "    def forward(self, x):\n",
    "        norm_att = torch.softmax(torch.tanh(self.att(x)), dim=-1)\n",
    "        cla = self.nonlinear_transform(self.cla(x))\n",
    "        x = torch.sum(norm_att * cla, dim=2)\n",
    "        return x, norm_att, cla\n",
    "\n",
    "    def nonlinear_transform(self, x):\n",
    "        if self.activation == 'linear':\n",
    "            return x\n",
    "        elif self.activation == 'sigmoid':\n",
    "            return torch.sigmoid(x)\n",
    "\n",
    "\n",
    "class TimmSED(nn.Module):\n",
    "    def __init__(self, base_model_name: str, pretrained=False, num_classes=24, in_channels=1, n_mels=24):\n",
    "        super().__init__()\n",
    "\n",
    "        self.bn0 = nn.BatchNorm2d(n_mels)\n",
    "\n",
    "        base_model = timm.create_model(\n",
    "            base_model_name, pretrained=pretrained, in_chans=in_channels)\n",
    "        layers = list(base_model.children())[:-2]\n",
    "        self.encoder = nn.Sequential(*layers)\n",
    "\n",
    "        in_features = base_model.num_features\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, in_features, bias=True)\n",
    "        self.att_block2 = AttBlockV2(\n",
    "            in_features, num_classes, activation=\"sigmoid\")\n",
    "\n",
    "        self.init_weight()\n",
    "\n",
    "    def init_weight(self):\n",
    "        init_bn(self.bn0)\n",
    "        init_layer(self.fc1)\n",
    "        \n",
    "\n",
    "    def forward(self, input_data):\n",
    "        x = input_data.transpose(2,3)\n",
    "        x = torch.cat((x,x,x),1)\n",
    "\n",
    "        x = x.transpose(2, 3)\n",
    "\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        x = torch.mean(x, dim=2)\n",
    "\n",
    "        x1 = F.max_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x2 = F.avg_pool1d(x, kernel_size=3, stride=1, padding=1)\n",
    "        x = x1 + x2\n",
    "\n",
    "        x = x.transpose(1, 2)\n",
    "        x = F.relu_(self.fc1(x))\n",
    "        x = x.transpose(1, 2)\n",
    "\n",
    "        (clipwise_output, norm_att, segmentwise_output) = self.att_block2(x)\n",
    "        logit = torch.sum(norm_att * self.att_block2.cla(x), dim=2)\n",
    "\n",
    "        output_dict = {\n",
    "            'logit': logit,\n",
    "        }\n",
    "\n",
    "        return output_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.841659Z",
     "iopub.status.busy": "2025-05-24T05:02:10.841249Z",
     "iopub.status.idle": "2025-05-24T05:02:10.862093Z",
     "shell.execute_reply": "2025-05-24T05:02:10.860881Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.841619Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "base_model_name='eca_nfnet_l0'\n",
    "pretrained=False\n",
    "in_channels=3\n",
    "\n",
    "MODELS = [f'/kaggle/input/birdclef-2025-sed-models-p/sed{i}.pth' for i in range(3)]\n",
    "\n",
    "MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:10.863659Z",
     "iopub.status.busy": "2025-05-24T05:02:10.863278Z",
     "iopub.status.idle": "2025-05-24T05:02:15.048035Z",
     "shell.execute_reply": "2025-05-24T05:02:15.046939Z",
     "shell.execute_reply.started": "2025-05-24T05:02:10.863622Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for path in MODELS:\n",
    "    model = TimmSED(base_model_name=base_model_name,\n",
    "               pretrained=pretrained,\n",
    "               num_classes=len(class_labels),\n",
    "               in_channels=in_channels,\n",
    "               n_mels=n_mels);\n",
    "    model.load_state_dict(torch.load(path, weights_only=True, map_location=torch.device('cpu')))\n",
    "    model.eval();\n",
    "    models.append(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.052399Z",
     "iopub.status.busy": "2025-05-24T05:02:15.052094Z",
     "iopub.status.idle": "2025-05-24T05:02:15.059094Z",
     "shell.execute_reply": "2025-05-24T05:02:15.058056Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.052374Z"
    },
    "papermill": {
     "duration": 0.011209,
     "end_time": "2025-03-12T14:13:02.593243",
     "exception": false,
     "start_time": "2025-03-12T14:13:02.582034",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def prediction(afile):    \n",
    "    global pred\n",
    "    path = test_audio_dir + afile + '.ogg'\n",
    "    with torch.inference_mode():\n",
    "        sig = audio_to_mel(path)\n",
    "        outputs = None\n",
    "        for model in models:\n",
    "            model.eval()\n",
    "            p = model(sig)\n",
    "            p = torch.sigmoid(p['logit']).detach().cpu().numpy() \n",
    "            p = apply_power_to_low_ranked_cols(p, top_k=30,exponent=2)\n",
    "            if outputs is None: outputs = p\n",
    "            else: outputs += p\n",
    "            \n",
    "        outputs /= len(models)\n",
    "        chunks = [[] for i in range(12)]\n",
    "        for i in range(len(chunks)):        \n",
    "            chunk_end_time = (i + 1) * 5\n",
    "            row_id = afile + '_' + str(chunk_end_time)\n",
    "            pred['row_id'].append(row_id)\n",
    "            bird_no = 0\n",
    "            for bird in class_labels:         \n",
    "                pred[bird].append(outputs[i,bird_no])\n",
    "                bird_no += 1\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.061548Z",
     "iopub.status.busy": "2025-05-24T05:02:15.060983Z",
     "iopub.status.idle": "2025-05-24T05:02:15.079172Z",
     "shell.execute_reply": "2025-05-24T05:02:15.077967Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.061509Z"
    },
    "papermill": {
     "duration": 6.823541,
     "end_time": "2025-03-12T14:13:09.419521",
     "exception": false,
     "start_time": "2025-03-12T14:13:02.59598",
     "status": "completed"
    },
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pred = {'row_id': []}\n",
    "for species_code in class_labels:\n",
    "    pred[species_code] = []\n",
    "    \n",
    "start = time.time()\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    _ = list(executor.map(prediction, file_list))\n",
    "end_t = time.time()\n",
    "\n",
    "if debug == True:\n",
    "    print(700*(end_t - start)/60/debug_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.080942Z",
     "iopub.status.busy": "2025-05-24T05:02:15.080536Z",
     "iopub.status.idle": "2025-05-24T05:02:15.123227Z",
     "shell.execute_reply": "2025-05-24T05:02:15.122134Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.080888Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results = pd.DataFrame(pred, columns = ['row_id'] + class_labels) \n",
    "display(results.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.124629Z",
     "iopub.status.busy": "2025-05-24T05:02:15.124231Z",
     "iopub.status.idle": "2025-05-24T05:02:15.158020Z",
     "shell.execute_reply": "2025-05-24T05:02:15.156970Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.124588Z"
    },
    "papermill": {
     "duration": 0.097214,
     "end_time": "2025-03-12T14:13:09.519812",
     "exception": false,
     "start_time": "2025-03-12T14:13:09.422598",
     "status": "completed"
    },
    "scrolled": true,
    "tags": [],
    "trusted": true
   },
   "outputs": [],
   "source": [
    "results.to_csv(\"submission.csv\", index=False)    \n",
    "\n",
    "sub = pd.read_csv('submission.csv')\n",
    "cols = sub.columns[1:]\n",
    "groups = sub['row_id'].str.rsplit('_', n=1).str[0]\n",
    "groups = groups.values\n",
    "for group in np.unique(groups):\n",
    "    sub_group = sub[group == groups]\n",
    "    predictions = sub_group[cols].values\n",
    "    new_predictions = predictions.copy()\n",
    "    for i in range(1, predictions.shape[0]-1):\n",
    "        new_predictions[i] = (predictions[i-1] * 0.2) + (predictions[i] * 0.6) + (predictions[i+1] * 0.2)\n",
    "    new_predictions[0] = (predictions[0] * 0.9) + (predictions[1] * 0.1)\n",
    "    new_predictions[-1] = (predictions[-1] * 0.9) + (predictions[-2] * 0.1)\n",
    "    sub_group[cols] = new_predictions\n",
    "    sub[group == groups] = sub_group\n",
    "sub.to_csv(\"submission.csv\", index=False)\n",
    "\n",
    "\n",
    "if debug:\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"color: #6cb4e4;  text-align: center;  padding: 0.25em;  border-top: solid 2.5px #6cb4e4;  border-bottom: solid 2.5px #6cb4e4;  background: -webkit-repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);  background: repeating-linear-gradient(-45deg, #f0f8ff, #f0f8ff 3px,#e9f4ff 3px, #e9f4ff 7px);height:45px;\">\n",
    "<b>\n",
    "ã€Šã€Šã€ŠFinaly Blendingã€‹ã€‹ã€‹\n",
    "</b></h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.159381Z",
     "iopub.status.busy": "2025-05-24T05:02:15.159070Z",
     "iopub.status.idle": "2025-05-24T05:02:15.163566Z",
     "shell.execute_reply": "2025-05-24T05:02:15.162494Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.159342Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------- #\n",
    "# [IMPORTANT]\n",
    "# * Blending Weight\n",
    "# ------------------------------------------ #\n",
    "sub_w=[0.4,0.6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-24T05:02:15.165080Z",
     "iopub.status.busy": "2025-05-24T05:02:15.164680Z",
     "iopub.status.idle": "2025-05-24T05:02:16.978680Z",
     "shell.execute_reply": "2025-05-24T05:02:16.977575Z",
     "shell.execute_reply.started": "2025-05-24T05:02:15.165043Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "list_TARGETs = sorted(os.listdir('/kaggle/input/birdclef-2025/train_audio/'))\n",
    "list_targets_0 = [f'{TARGET} 0' for TARGET in list_TARGETs]\n",
    "list_targets_1 = [f'{TARGET} 1' for TARGET in list_TARGETs]\n",
    "\n",
    "df0 = pd.read_csv(\"/kaggle/working/submission.csv\")\n",
    "df1 = pd.read_csv(\"/kaggle/working/submission1.csv\")\n",
    "\n",
    "df0 = df0.rename(columns={TARGET : f'{TARGET} 0' for TARGET in list_TARGETs})\n",
    "df1 = df1.rename(columns={TARGET : f'{TARGET} 1' for TARGET in list_TARGETs})\n",
    "\n",
    "dfs = pd.merge(df0,df1,on=['row_id'])\n",
    "\n",
    "for i in range(len(list_TARGETs)):\n",
    "    dfs[list_TARGETs[i]] = dfs[list_targets_0[i]]*sub_w[0] + sub_w[1]*dfs[list_targets_1[i]]\n",
    "             \n",
    "for col0,col1 in zip(list_targets_0, list_targets_1):\n",
    "    del dfs[col0]\n",
    "    del dfs[col1]\n",
    "    \n",
    "    \n",
    "dfs.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 11361821,
     "isSourceIdPinned": false,
     "sourceId": 91844,
     "sourceType": "competition"
    },
    {
     "datasetId": 7430593,
     "sourceId": 11828260,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7457365,
     "sourceId": 11867185,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7459867,
     "sourceId": 11870659,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27.447062,
   "end_time": "2025-03-12T14:13:11.647927",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-12T14:12:44.200865",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
